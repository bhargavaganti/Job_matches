{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Job Description to Job Titles: Machine Learning Mechanism\n",
    "\n",
    "### Strayn Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input dataset here contains two parts: a) cleaned job posting as X, which has only the extracted information of required technical skills, job responsibilities, job qualifications; b) cleaned job titles as Y, whose stopwords, irrelevant description and job-level components are all removed (but the descriptive component and objective component haven't been seperated).   \n",
    "**X**: Job descriptions are encoded to sqeuences, and post-padded with 0s, then serialize with time step of 15.  \n",
    "**Y**: The job labels are encoded with [fastText](https://github.com/facebookresearch/fastText).  \n",
    "I randomly sampled 80% of original dataset to form training dataset and leave the left to be testing dataset.  \n",
    "**training**: I chose to minimize cosine proximity loss between vectorized job description (by a trainable LSTM) and vectorized job titles (by a un-trainable fastText).  \n",
    "I tested 21 LSTM variants to find out optimal structures to vectorize job description, among them I chose the LSTM-19, which has the following structures:    \n",
    "```\n",
    "def create_LSTM(input_dim,output_dim,time_steps=10,embedding_matrix=[]):\n",
    "    batch_size = 1\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    inputs = Input(shape=(batch_size,time_steps, input_dim))\n",
    "    if embedding_matrix != []:\n",
    "        embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                                    embedding_matrix.shape[1],\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_shape=(input_dim,),\n",
    "                                    trainable=False)\n",
    "        x = embedding_layer(inputs)\n",
    "        x = Reshape([embedding_matrix.shape[1],input_dim])(x)\n",
    "    else:\n",
    "        x = Reshape([time_steps,input_dim])(inputs)\n",
    "    \n",
    "    x = Bidirectional(LSTM(100, return_sequences=True))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = attention_3d_block(x,input_dim=200)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    #LSTM OUT\n",
    "    x = Bidirectional(LSTM(150))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    #NN OUT\n",
    "    x = Dense(output_dim, activation='tanh')(x)\n",
    "    model = Model(input=inputs, output=x)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "```\n",
    "**evaluation**: Except for looking at the predicted titles by human intuition, I made a score called *Ranking Percentage Score*, which is the percentage of correct label located in the prediction sequence, which is ordered by predicted cosine proximity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyartsgnaw/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model imported\n",
      "load_model imported\n",
      "get_rank_df imported\n",
      "Word2Vec imported\n",
      "WordNetLemmatizer imported\n",
      "get_similar_words imported\n",
      "get_time_series imported\n",
      "nltk imported\n",
      "prepare_data imported\n",
      "remove_stopwords imported\n",
      "sent_tokenize imported\n",
      "stopwords imported\n",
      "wn imported\n",
      "word_tokenize imported\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "try:\n",
    "\tCWDIR = os.path.abspath(os.path.dirname(__file__))\n",
    "except:\n",
    "\tCWDIR = os.getcwd()\t\n",
    "\n",
    "from keras import metrics\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "def import_local_package(addr_pkg,function_list=[]):\n",
    "\t#import local package by address\n",
    "\t#it has to be imported directly in the file that contains functions required the package, i.e. it cannot be imported by from .../utils import import_local_package\n",
    "\timport importlib.util\n",
    "\tspec = importlib.util.spec_from_file_location('pkg', addr_pkg)\n",
    "\tmyModule = importlib.util.module_from_spec(spec)\n",
    "\tspec.loader.exec_module(myModule)\n",
    "\tif len(function_list)==0:\n",
    "\t\timport re\n",
    "\t\tfunction_list = [re.search('^[a-zA-Z]*.*',x).group() for x in dir(myModule) if re.search('^[a-zA-Z]',x) != None]\n",
    "\n",
    "\tfor _f in function_list:\n",
    "\t\ttry:\n",
    "\t\t\teval(_f)\n",
    "\t\texcept NameError:\n",
    "\t\t\texec(\"global {}; {} = getattr(myModule,'{}')\".format(_f,_f,_f)) #exec in function has to use global in 1 line\n",
    "\t\t\tprint(\"{} imported\".format(_f))\n",
    "\n",
    "\treturn\n",
    "import_local_package(os.path.join(CWDIR,'./lib/utils.py'),['train_model','load_model','get_rank_df'])\n",
    "import_local_package(os.path.join(CWDIR,'./data/lib/prepare_data.py'),[])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXP_ID                         35\n",
      "N_EPOCH                       200\n",
      "PATIENCE                      100\n",
      "IS_TRAIN                        1\n",
      "LOSS_FUNC        cosine_proximity\n",
      "OUTPUT_DIM                    200\n",
      "INPUT_DIM                     200\n",
      "TIME_STEPS                     15\n",
      "MODEL_ID                  LSTM_19\n",
      "IS_RUN                          0\n",
      "RANK_SCORE               0.506717\n",
      "QUIT_LOSS               -0.876086\n",
      "QUIT_MSE               0.00585117\n",
      "QUIT_EPOCH                    199\n",
      "N_PARAMS                   765200\n",
      "start_time    2018-08-06 20:03:05\n",
      "end_time      2018-08-06 21:32:46\n",
      "note                      Bi-LSTM\n",
      "Name: 35, dtype: object\n",
      "Activation imported\n",
      "BatchNormalization imported\n",
      "Bidirectional imported\n",
      "Conv1D imported\n",
      "Conv2D imported\n",
      "Conv2DTranspose imported\n",
      "Convolution3D imported\n",
      "Dense imported\n",
      "Dropout imported\n",
      "Embedding imported\n",
      "Flatten imported\n",
      "GaussianDropout imported\n",
      "GaussianNoise imported\n",
      "Input imported\n",
      "K imported\n",
      "LSTM imported\n",
      "Lambda imported\n",
      "LeakyReLU imported\n",
      "MaxPooling2D imported\n",
      "Model imported\n",
      "Permute imported\n",
      "RepeatVector imported\n",
      "Reshape imported\n",
      "Sequential imported\n",
      "TimeDistributed imported\n",
      "UpSampling1D imported\n",
      "UpSampling2D imported\n",
      "UpSampling3D imported\n",
      "attention_3d_block imported\n",
      "create_LSTM imported\n",
      "initializers imported\n",
      "merge imported\n",
      "regularizers imported\n"
     ]
    }
   ],
   "source": [
    "# inputs\n",
    "path_exp = os.path.join(CWDIR,'./experiments/exp_logs.xlsx')\n",
    "df_exp = pd.read_excel(path_exp)\n",
    "with open(os.path.join(CWDIR,'./experiments/.idx'),'r') as f:\n",
    "    idx = int(f.read())\n",
    "idx =35\n",
    "\n",
    "exp = df_exp.iloc[idx]\n",
    "\n",
    "# setup model parameters\n",
    "start_time = datetime.datetime.now()\n",
    "EXP_ID = exp['EXP_ID'] #the name for this experiment \n",
    "MODEL_ID = exp['MODEL_ID'] #model framework\n",
    "OUTPUT_DIM = int(exp['OUTPUT_DIM']) # LSTM output vector dimension, should match that of Word2Vec of labels\n",
    "INPUT_DIM = int(exp['INPUT_DIM']) # LSTM input vector dimension, length of tokens cut from original data texts for each record\n",
    "TIME_STEPS = int(exp['TIME_STEPS']) #for LSTM sequential\n",
    "N_EPOCH = int(exp['N_EPOCH']) #for LSTM\n",
    "PATIENCE = int(exp['PATIENCE']) #for LSTM\n",
    "TRAIN_MODEL = int(exp['IS_TRAIN'])\n",
    "LOSS=exp['LOSS_FUNC']\n",
    "print(exp)\n",
    "import_local_package(os.path.join(CWDIR,'./experiments/models/{}.py'.format(MODEL_ID)),[])\n",
    "\n",
    "# setup logging address\n",
    "path_vectors = os.path.join(CWDIR,'./logs/models/vectors_JT-{}.csv'.format(INPUT_DIM))\n",
    "path_model = os.path.join(CWDIR,'./logs/models/LSTM_{}.model'.format(EXP_ID))\n",
    "path_eval = os.path.join(CWDIR,'./logs/eval/LSTM_eval_{}.csv'.format(EXP_ID))\n",
    "path_training_model = os.path.join(CWDIR,'./logs/models/LSTM_train_{}.model'.format(EXP_ID))\n",
    "path_training_log = os.path.join(CWDIR,'./logs/train_logs/LSTM_logs{}.csv'.format(EXP_ID))\n",
    "#\tif os.path.isfile(path_training_log):\n",
    "#\t\tos.remove(path_training_log)\n",
    "os.system('mkdir -p {}'.format(os.path.join(CWDIR,'./logs/eval/')))\n",
    "os.system('mkdir -p {}'.format(os.path.join(CWDIR,'./logs/models/')))\n",
    "os.system('mkdir -p {}'.format(os.path.join(CWDIR,'./logs/train_logs/')))\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load/prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the label Ys\n",
    "path_vectors = os.path.join(CWDIR,'./logs/models/vectors_JT-{}.csv'.format(INPUT_DIM))\n",
    "if not os.path.isfile(path_vectors):\n",
    "    os.system('python {}'.format(os.path.join(CWDIR,'./lib/train_fasttext.py')))\n",
    "\n",
    "labels = pd.read_csv(path_vectors).values\n",
    "\n",
    "# read the data Xs\n",
    "path_data = os.path.join(CWDIR,'./data/df_all.csv')\n",
    "df_all = pd.read_csv(path_data)\n",
    "\n",
    "# encode the data into sequence\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([' '.join(df_all['texts'])])\n",
    "data = tokenizer.texts_to_sequences(df_all['texts'])\n",
    "data = sequence.pad_sequences(data, padding='post',truncating='post',maxlen=INPUT_DIM) # truncate and pad input sequences\n",
    "\n",
    "# prepare trainig/testing data/labels\n",
    "judge = (df_all['split']=='train').values\n",
    "train_data = data[judge]\n",
    "test_data = data[~judge]\n",
    "train_labels = labels[judge]\n",
    "test_labels = labels[~judge]\n",
    "\n",
    "# serialize the data/labels to model input/output format\n",
    "X_train, Y_train = get_time_series(train_data,train_labels,TIME_STEPS,0)\n",
    "X_test, Y_test = get_time_series(test_data,test_labels,TIME_STEPS,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyartsgnaw/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1271: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "1\n",
      "Train on 12436 samples, validate on 1382 samples\n",
      "Epoch 1/1\n",
      " - 28s - loss: -7.0686e-01 - mean_squared_error: 0.0069 - cosine_proximity: -7.0686e-01 - val_loss: -6.5758e-01 - val_mean_squared_error: 0.4877 - val_cosine_proximity: -6.5758e-01\n"
     ]
    }
   ],
   "source": [
    "# create/load the model    \n",
    "from keras.models import load_model\n",
    "if os.path.isfile(path_model):\n",
    "    model = load_model(path_model)\n",
    "else:\n",
    "    embedding_matrix = []\n",
    "#        embedding_matrix = load_embedding_fasttext(path_JD)\n",
    "#       model = create_LSTM(input_dim=INPUT_DIM,output_dim=OUTPUT_DIM,embedding_matrix=embedding_matrix)\n",
    "    model = create_LSTM(input_dim=INPUT_DIM,output_dim=OUTPUT_DIM,time_steps=TIME_STEPS,embedding_matrix=embedding_matrix)\n",
    "# train the model\n",
    "N_EPOCH = 1\n",
    "if TRAIN_MODEL == True:\n",
    "    adam=Adam(lr=0.005, beta_1=0.9 ,decay=0.001)\n",
    "    model.compile(loss=LOSS, optimizer=adam, metrics=['mse','cosine_proximity'])\n",
    "    model = train_model(model,X_train=X_train.reshape([-1,1,TIME_STEPS,INPUT_DIM]),\\\n",
    "                        Y_train=Y_train,\\\n",
    "                        verbose=1,n_epoch=N_EPOCH,validation_split=0.1,patience=PATIENCE,\\\n",
    "                        model_path=path_training_model,\n",
    "                        log_path=path_training_log)\n",
    "    model.save(path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job: full-time community connections intern paid internship\n",
      "   software engineer\n",
      "   data administrator 1 it division financial monitoring center\n",
      "   radio optimization senior engineer\n",
      "   head financial department\n",
      "   safety manager\n",
      "   procurement specialist\n",
      "   sales senior specialist commercial directorate\n",
      "   web developer\n",
      "   domestic expert international exposure accounting\n",
      "   human resources senior specialist\n",
      "Percentage_Rank of 0: 0.7865214116010895\n",
      "\n",
      "Job: bcc specialist\n",
      "   software engineer\n",
      "   data administrator 1 it division financial monitoring center\n",
      "   radio optimization senior engineer\n",
      "   head financial department\n",
      "   safety manager\n",
      "   procurement specialist\n",
      "   sales senior specialist commercial directorate\n",
      "   domestic expert international exposure accounting\n",
      "   human resources senior specialist\n",
      "   education officer\n",
      "Percentage_Rank of 1: 0.7081184446891117\n",
      "\n",
      "Job: chauffeur fsn-3 fp-bb*\n",
      "   software engineer\n",
      "   data administrator 1 it division financial monitoring center\n",
      "   radio optimization senior engineer\n",
      "   head financial department\n",
      "   safety manager\n",
      "   procurement specialist\n",
      "   sales senior specialist commercial directorate\n",
      "   domestic expert international exposure accounting\n",
      "   human resources senior specialist\n",
      "   education officer\n",
      "Percentage_Rank of 2: 0.9626818102798864\n",
      "\n",
      "Job: demographic analysis workshop\n",
      "   software engineer\n",
      "   data administrator 1 it division financial monitoring center\n",
      "   radio optimization senior engineer\n",
      "   head financial department\n",
      "   safety manager\n",
      "   procurement specialist\n",
      "   sales senior specialist commercial directorate\n",
      "   human resources senior specialist\n",
      "   education officer\n",
      "   administrative assistant\n",
      "Percentage_Rank of 3: 0.8447586486643102\n",
      "\n",
      "Job: receptionist\n",
      "   software engineer\n",
      "   data administrator 1 it division financial monitoring center\n",
      "   radio optimization senior engineer\n",
      "   head financial department\n",
      "   safety manager\n",
      "   procurement specialist\n",
      "   sales senior specialist commercial directorate\n",
      "   technical writer\n",
      "   domestic expert international exposure accounting\n",
      "   web developer\n",
      "Percentage_Rank of 4: 0.19070522106971083\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model by ranking percentage score\n",
    "yhat = model.predict(X_test.reshape([-1,1,TIME_STEPS,INPUT_DIM]))[:5]\n",
    "# prepare the original testing labels\n",
    "titles_all = df_all.titles.values\n",
    "titles_test = titles_all[~judge][:5]\n",
    "Y = np.concatenate([Y_test,Y_train])\n",
    "\n",
    "df = get_rank_df(yhat,titles_test,Y,titles_all)\n",
    "#\tdf = get_rank_df(yhat,titles_test,Y_test,titles_test)\n",
    "df.to_csv(path_eval,index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Log the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error tokenizing data. C error: Expected 5 fields in line 202, saw 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# log the results\n",
    "exp['start_time'] = str(start_time.replace(microsecond=0))\n",
    "exp['end_time'] = str(datetime.datetime.now().replace(microsecond=0))\n",
    "\n",
    "try:\n",
    "    exp['RANK_SCORE'] = df['rank'].mean()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "try:\n",
    "    df_log = pd.read_csv(path_training_log)\n",
    "    exp['QUIT_LOSS'] = df_log.iloc[-1]['loss']\n",
    "    exp['QUIT_EPOCH'] = df_log.iloc[-1]['epoch']\n",
    "    exp['QUIT_MSE'] = df_log.iloc[-1]['mean_squared_error']\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    exp['N_PARAMS'] = model.count_params()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
